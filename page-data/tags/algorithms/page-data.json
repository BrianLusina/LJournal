{"componentChunkName":"component---src-templates-tags-tags-jsx","path":"/tags/algorithms/","result":{"data":{"allMarkdownRemark":{"totalCount":8,"edges":[{"node":{"frontmatter":{"title":"Bottom up algorithms","subtitle":"A short description on bottom up algorithms","excerpt":"Going bottom-up is a way to avoid recursion, saving the memory cost that recursion incurs when it builds up the call stack.","path":"/tech/bottom-up-algorithms","date":"May 18, 2017","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":null,"thumbnail":"bottom-up.png","teaser":"bottom-up.png","credit":null,"creditlink":null},"tags":["data-structures","algorithms"]},"excerpt":"Going bottom-up is a way to avoid recursion, saving the memory cost that recursion incurs when it builds up the call stack. Put simply, a…","timeToRead":1,"html":"<p>Going bottom-up is a way to avoid recursion, saving the memory cost that recursion incurs when it builds up the call stack.</p>\n<p>Put simply, a bottom-up algorithm \"starts from the beginning,\" while a recursive algorithm often \"starts from the end and works backwards.\"</p>\n<p>For example, if we wanted to multiply all the numbers in the range 1...n1...n, we could use this cute, top-down, recursive one-liner:</p>\n<pre><code class=\"language-python\">def product_1_to_n(n):\n    # we assume n >= 1\n    return n * product_1_to_n(n-1) if n > 1 else 1\n</code></pre>\n<p>This approach has a problem: it builds up a call stack of size O(n)O(n), which makes our total memory cost O(n)O(n). This makes it vulnerable to a stack overflow error, where the call stack gets too big and runs out of space.</p>\n<p>To avoid this, we can instead go bottom-up:</p>\n<pre><code class=\"language-python\">def product_1_to_n(n):\n    # we assume n >= 1\n\n    result = 1\n    for num in range(1, n+1):\n        result *= num\n\n    return result\n</code></pre>\n<p>This approach uses O(1)O(1) space (O(n)O(n) time).</p>\n<blockquote>\n<p>Some compilers and interpreters will do what's called tail call optimization (TCO), where it can optimize some recursive functions to avoid building up a tall call stack. Python and Java decidedly do not use TCO. Some Ruby implementations do, but most don't. Some C implementations do, and the JavaScript spec recently allowed TCO. Scheme is one of the few languages that guarantee TCO in all implementations. In general, best not to assume your compiler/interpreter will do this work for you.</p>\n</blockquote>\n<p>Going bottom-up is a common strategy for dynamic programming problems, which are problems where the solution is composed of solutions to the same problem with smaller inputs (as with the fibonacci problem, above). The other common strategy for dynamic programming problems is memoization.</p>"}},{"node":{"frontmatter":{"title":"Concept of Memoize","subtitle":"A gentle introduction to memoize","excerpt":"Memoization ensures that a function doesn't run for the same inputs more than once by keeping a record of the results for given inputs (usually in a dictionary).","path":"/tech/memoize-python","date":"May 17, 2017","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":null,"thumbnail":"memoize.png","teaser":"memoize.png","credit":null,"creditlink":null},"tags":["data-structures","python","algorithms","memoize"]},"excerpt":"Memoization ensures that a function doesn't run for the same inputs more than once by keeping a record of the results for given inputs…","timeToRead":2,"html":"<p>Memoization ensures that a function doesn't run for the same inputs more than once by keeping a record of the results for given inputs (usually in a dictionary).</p>\n<p>For example, a simple recursive function for computing the n<sup>th</sup> fibonacci number:</p>\n<pre><code class=\"language-python\">def fib_recursive(n):\n    if n &#x3C; 0:\n        raise IndexError('Index was negative. No such thing as a negative index in a series.')\n\n    # base cases\n    if n in [0, 1]:\n        return n\n\n    print \"computing fib_recursive(%i)\" % n\n    return fib_recursive(n - 1) + fib_recursive(n - 2)\n</code></pre>\n<p>This will run the same input a couple of times</p>\n<pre><code class=\"language-bash\">>>> fib_recursive(8)\ncomputing fib_recursive(8)\ncomputing fib_recursive(7)\ncomputing fib_recursive(6)\ncomputing fib_recursive(5)\ncomputing fib_recursive(4)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(2)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(4)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(2)\ncomputing fib_recursive(5)\ncomputing fib_recursive(4)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(2)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(6)\ncomputing fib_recursive(5)\ncomputing fib_recursive(4)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(2)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(4)\ncomputing fib_recursive(3)\ncomputing fib_recursive(2)\ncomputing fib_recursive(2)\n21\n</code></pre>\n<p>We can imagine the recursive calls of this function as a tree, where the two children of a node are the two recursive calls it makes. We can see that the tree quickly branches out of control:</p>\n<p><img src=\"https://www.interviewcake.com/images/svgs/fibonacci__binary_tree_recursive.svg?bust=145\"></p>\n<p>To avoid the duplicate work caused by the branching, we can wrap the function in a class that stores an instance variable, memo, that maps inputs to outputs. Then we simply:</p>\n<ol>\n<li>Check memo to see if we can avoid computing the answer for any given input, and</li>\n<li>Save the results of any calculations to memo.</li>\n</ol>\n<pre><code class=\"language-python\">class Fibber:\n    def __init__(self):\n        self.memo = {}\n\n    def fib(self, n):\n\n        if n &#x3C; 0:\n            raise Exception(\"Index was negative. No such thing as a negative index in a series.\")\n\n        # base cases\n        elif n in [0, 1]:\n            return n\n\n        # see if we've already calculated this\n        if n in self.memo:\n            print \"grabbing memo[%i]\" % n\n            return self.memo[n]\n\n        print \"computing fib(%i)\" % n\n        result = self.fib(n - 1) + self.fib(n - 2)\n\n        # memoize\n        self.memo[n] = result\n\n        return result\n</code></pre>\n<p>We save a bunch of calls by checking the memo:</p>\n<pre><code class=\"language-python\">>>> Fibber().fib(8)\ncomputing fib(8)\ncomputing fib(7)\ncomputing fib(6)\ncomputing fib(5)\ncomputing fib(4)\ncomputing fib(3)\ncomputing fib(2)\ngrabbing memo[2]\ngrabbing memo[3]\ngrabbing memo[4]\ngrabbing memo[5]\ngrabbing memo[6]\n21\n</code></pre>\n<p>Now in our recurrence tree, no node appears more than twice:</p>\n<p><img src=\"https://www.interviewcake.com/images/svgs/fibonacci__binary_tree_memoized.svg?bust=145\"></p>\n<p>Memoization is a common strategy for dynamic programming problems, which are problems where the solution is composed of solutions to the same problem with smaller inputs (as with the fibonacci problem, above). The other common strategy for dynamic programming problems is going bottom-up, which is usually cleaner and often more efficient.</p>"}},{"node":{"frontmatter":{"title":"Perfect Binary Tree","subtitle":"A gentle introduction to perfect binary trees","excerpt":"Binary trees are part of a data structure known as `Trees`, yeah, I know, the forefathers of computer science and software engineering were quite creative. They were quite observant. Trees, more or less, look like trees, like the literal trees in nature. Now, that we have the most basic and almost worst analogy out there, what are binary trees?","path":"/tech/binary-tree","date":"January 26, 2017","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"binary_tree.png","thumbnail":"binary_tree.png","teaser":"binary_tree.png","credit":"Quora","creditlink":null},"tags":["algorithms","data-structures","binary-tree"]},"excerpt":"Binary trees are part of a data structure known as Trees, yeah, I know, the forefathers of computer science and software engineering were…","timeToRead":4,"html":"<p>Binary trees are part of a data structure known as <code>Trees</code>, yeah, I know, the forefathers of computer science and software engineering were quite creative. They were quite observant. Trees, more or less, look like trees, like the literal trees in nature. Now, that we have the most basic and almost worst analogy out there, what are binary trees?</p>\n<p>I shall give a basic and brief introduction into <code>binary trees</code>, this will assume that you have knowledge on tree data structures. Even if you don't, you can still read on, they are all pretty much related, so the knowledge is transferable.</p>\n<p>A binary tree is a <code>tree</code> where every node has 2 or fewer children. The children are usually called <code>left</code> and <code>right</code>.\nA simple implementation in <code>Python</code>:</p>\n<pre><code class=\"language-python\">class BinaryTreeNode(object):\n    def __init__(self, value):\n        self.value = value\n         self.left = None\n         self.right = None\n</code></pre>\n<blockquote>\n<p>A class definition for a Binary Tree Node in Python</p>\n</blockquote>\n<p>In JavaScript:</p>\n<pre><code class=\"language-javascript\">function BinaryTreeNode(value) {\n  this.value = value\n  this.left = null\n  this.right = null\n}\n</code></pre>\n<blockquote>\n<p>An object definition in JavaScript</p>\n</blockquote>\n<p>In Java:</p>\n<pre><code class=\"language-java\">public class &#x3C;T> BinaryTreeNode{\n    public T value;\n    public BinaryTreeNode left;\n    public BinaryTreeNode right;\n\n    public BinaryTreeNode(T value){\n        this.value = value;\n    }\n}\n</code></pre>\n<blockquote>\n<p>Binary Tree node implementation of Binary Tree node in Java, this uses T to define objects.</p>\n</blockquote>\n<p>And finally in Ruby:</p>\n<pre><code class=\"language-ruby\">class BinaryTreeNode\n\n    attr_accessor :value, :left, :right\n\n    def initialize(value)\n        @value = value\n        @left  = nil\n        @right = nil\n    end\nend\n</code></pre>\n<p>Why these languages? Well, because I am familiar with them and also to show that it does not matter the language, the implementation of a data structure is usually the same.</p>\n<p><img src=\"https://www.interviewcake.com/images/svgs/binary_tree__depth_5.svg?bust=138\" alt=\"binary tree\"></p>\n<blockquote>\n<p>Binary tree data structure, you will notice, it pretty much looks like a tree.</p>\n</blockquote>\n<p>This particular example is special because every level of the tree is completely full. There are no \"gaps.\" We call this kind of tree \"perfect.\"</p>\n<p>Binary trees have a few interesting properties when they're perfect:</p>\n<ol>\n<li>The number of total nodes on <em>each</em> doubles as we move down the tree\n<img src=\"https://www.interviewcake.com/images/svgs/binary_tree__depth_5_with_number_of_nodes_labelled.svg?bust=138\" alt=\"perfectBinaryTree\"></li>\n<li>\n<p>The number of nodes on the last level is equal to the sum of all the nodes on other levels(plus 1)\nLet's call the number of nodes <code>n</code>, and the height of the tree <code>h</code>. <code>h</code> can also be thought of as the <em>number of levels</em>. If we had <code>h</code>, how could we calculate <code>n</code>?\nLet's just add up the number of nodes on each level! How many nodes are on each level?\nIf we zero-index the levels, the number of nodes on the <code>x</code>th level is exactly <code>2^x</code>!</p>\n<ul>\n<li>Level 0: 2^0 nodes</li>\n<li>Level 1: 2^1 nodes,</li>\n<li>Level 2: 2^2 nodes,</li>\n<li>Level 3: 2^3 nodes,</li>\n<li>etc</li>\n</ul>\n<p>So our total number of nodes is:\n$$n= 2^0 + 2^1 +2^2 +2^3 +...+2^{h−1}$$ > Why only up to 2^{h-1} ? Notice that we started counting our levels at 0. So if > we have h levels in total, the last level is actually the \"h−1\"-th level. > That means the number of nodes on the last level is 2^{h-1}.</p>\n<p>But we can simplify. Property 2 tells us that the number of nodes on the last level is (1 more than) half of the total number of nodes, so we can just take the number of nodes on the last level, multiply it by 2, and subtract 1 to get the number of nodes overall.\nWe know the number of nodes on the last level is 2^{h-1}, So:</p>\n<p>$$ n = 2^{h-1} * 2 - 1$$\n$$n = 2^{h−1}∗2^{1}−1$$\n$$n = 2^{h-1+1}- 1$$\n$$n = 2^{h} - 1$$</p>\n<p>So that's how we can go from <code>h</code> to <code>n</code>. What about the other direction?</p>\n<p>We need to bring the <code>h</code> down from the exponent. That's what logs are for!</p>\n<p>First, some quick review on Algebra. $$log<em>{10}(100)$$ simply means, \"What power must you raise 10 to in order to get 100?\". Which is 2, because $$10^2 = 100$$.\nWe can use logs in algebra to bring variables down from exponents by exploiting the fact that we can simplify $$log</em>{10}(10^2)$$. What power must we raise 10 to in order to get $$10^2$$?\nThat's easy — it's 2.</p>\n<p>So in this case we can take the $$log_{2}$$ of both sides:</p>\n<p>$$n = 2^{h} - 1$$\n$$n + 1 = 2^{h}$$\n$$log<em>{2}{((n+1))} = log</em>{2}{(2^{h})}$$\n$$log_{2}{(n+1)} = h$$</p>\n</li>\n</ol>\n<h3>Conclusion</h3>\n<p>So that's the relationship between height and total nodes in a perfect binary tree.\nI know it involved a bit of Math, which you did not want nor expect, however, this will make the programs you write faster and better and of course easier to maintain and debug.</p>\n<p>Let us plant more trees!</p>"}},{"node":{"frontmatter":{"title":"Vigenere Cipher","subtitle":"The Vigenere Cipher Broken down in Python","excerpt":"The Vigenere cipher is likely the the most secure ciphers out there. It builds on the principle of the Caesar cipher yet provides a decent way to avoid the easy to solve shift problems. The basic gist of this cipher is we have both a message and a key. The key can be any length, but you must repeat the key for the length of our message to get this to work","path":"/tech/vigenere-cipher","date":"September 07, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"vigenere_cipher.jpg","thumbnail":"vigenere_cipher.jpg","teaser":"vigenere_cipher.jpg","credit":"MTU","creditlink":"www.cs.mtu.edu"},"tags":["algorithms","puzzles","ciphers"]},"excerpt":"The Vigenere cipher is likely the the most secure ciphers out there. It builds on the principle of the Caesar cipher yet provides a decent…","timeToRead":4,"html":"<p>The Vigenere cipher is likely the the most secure ciphers out there. It builds on the principle of the Caesar cipher yet provides a decent way to avoid the easy to solve shift problems. The basic gist of this cipher is we have both a message and a key. The key can be any length, but you must repeat the key for the length of our message to get this to work. This can be seen here:</p>\n<pre><code class=\"language-plain\">alpha = ABCDEFGHIJKLMNOPQRSTUVWXYZ\nmessage  = IAMTHEWALRUS\nkey  = HELLOHELLOHE\n</code></pre>\n<p>Our key is actually \"HELLO\", but we expanded it to the length of our message giving us the repeated nature we see. Once we have these defined, we go character by character performing a pseudo-Caesar cipher.</p>\n<pre><code class=\"language-plain\">m1 = I = 9\nk1 = H = 8\n9 + 8 = 17 % 26 = 17 = Q\nc1 = Q\n</code></pre>\n<p>Looking at this, we see that the first character of our message is \"I\" which is the ninth letter in the alphabet. We then look at the first character of the key which is \"H\" or the eighth letter in the alphabet. We add those two numbers and modulo 26 giving us 17 which points to the seventeenth letter in the alphabet: \"Q\". We now know that the first letter of our ciphertext is \"Q\".</p>\n<p>We then repeat this method for each character in our message until we have the ciphertext. This can be better shown as the algorithm:</p>\n<p>Let m be our message and k be our key:\nE(m) = ((m1 + k1) % 26, (m2 + k2) % 26, ..., (mi + ki) % 26)\nD(m) = ((c1 - k1) % 26, (c2 - k2) % 26, ..., (ci - ki) % 26)\nThis is much like the Caesar cipher except instead of defining a fixed rotation, we allow our key's character index to be the rotation. As you can see, this is why the Vigenere cipher can be considered a string of Caesar ciphers. Pretty cool when you actually see it.</p>\n<p>So how do we attack this then?</p>\n<p>Well, the problem with this cipher is the fact that the key repeats itself. When you have a repeating key, it's common to see patterns in the ciphertext that completely match each other. By recognizing those patterns, you can determine the block size of the key and from there you simply do a Caesar brute force shift on each block until the plaintext appears.</p>\n<p>On relatively short messages, this is harder to crack (as with any short ciphertext) but if encrypting a uniformly distributed text then you can really start to pick up on these things.</p>\n<p>Now let's code this up in Python and see how how we can automate this:</p>\n<pre><code class=\"language-python\">from itertools import cycle\n\nALPHA = 'abcdefghijklmnopqrstuvwxyz'\n\n\ndef encrypt(key, plaintext):\n    \"\"\"Encrypt the string and return the ciphertext\"\"\"\n    pairs = zip(plaintext, cycle(key))\n    result = ''\n\n    for pair in pairs:\n        total = reduce(lambda x, y: ALPHA.index(x) + ALPHA.index(y), pair)\n        result += ALPHA[total % 26]\n\n    return result.lower()\n\n\ndef decrypt(key, ciphertext):\n    \"\"\"Decrypt the string and return the plaintext\"\"\"\n    pairs = zip(ciphertext, cycle(key))\n    result = ''\n\n    for pair in pairs:\n        total = reduce(lambda x, y: ALPHA.index(x) - ALPHA.index(y), pair)\n        result += ALPHA[total % 26]\n\n    return result\n\n\ndef show_result(plaintext, key):\n    \"\"\"Generate a resulting cipher with elements shown\"\"\"\n    encrypted = encrypt(key, plaintext)\n    decrypted = decrypt(key, encrypted)\n\n    print 'Key: %s' % key\n    print 'Plaintext: %s' % plaintext\n    print 'Encrytped: %s' % encrypted\n    print 'Decrytped: %s' % decrypted\n</code></pre>\n<h2>Step One</h2>\n<p>Import the <code>cycle()</code> function from the <code>itertools</code> library.</p>\n<p>Define our alphabet in order to get character indexes correctly, this can be done with the <code>string</code> module in Python, which enables us to get all the letters in the alphabet we need. This avoids the issue of forgetting a letter in case you hardcode the alphabet.</p>\n<p>The function <code>encrypt(key, plaintext)</code> takes in a key and a plain text, I build a tuple with the <code>zip()</code> function which is a terminating function. Which means it will stop as soon as the shorter string is exhausted.</p>\n<p>An example</p>\n<pre><code class=\"language-python\"># in\nlist(zip(string.ascii_lowercase, string.ascii_uppercase))\n\n# out\n[('a', 'A'), ('b', 'B'), ('c', 'C'), ('d', 'D'), ('e', 'E'), ('f', 'F'), ('g', 'G'), ('h', 'H'), ('i', 'I'), ('j', 'J'), ('k', 'K'), ('l', 'L'), ('m', 'M'), ('n', 'N'), ('o', 'O'), ('p', 'P'), ('q', 'Q'), ('r', 'R'), ('s', 'S'), ('t', 'T'), ('u', 'U'), ('v', 'V'), ('w', 'W'), ('x', 'X'), ('y', 'Y'), ('z', 'Z')]\n</code></pre>\n<p><code>cycle</code> is used to repeat the letters of the key for the entirety of the plaintext, note that this can repeat indefinately.</p>\n<p>Perform a loop in each of the pairs reducing them to a single value with <code>reduce</code> function from functools library. The <code>reduce</code> function takes in a function and an iterable object.</p>\n<p>The sum could be used here as well, but it would mean remembring that indexes of the letters and not the letters themselves are needed, or else we'll get a value error.</p>\n<p>Finally get the new letter after a modulo of 26 and append that to our resulting ciphertext string.</p>\n<h2>Step 2</h2>\n<p>Create the decipher function. This is fundamentally the same with the only difference being the fact that we subtract instead of adding the letters.</p>\n<h2>Step 3</h2>\n<p>Output these results. :)</p>\n<h2>Conclusion</h2>\n<p>In essence this is a Caeser's cipher with the only difference being the fact that we allow out key's character index to rotate instead of defining a fixed rotation. As you can see, this is why the Vigenere cipher can be considered a string of Caesar ciphers.</p>"}},{"node":{"frontmatter":{"title":"Big-O-Notation","subtitle":"The Big Deal with the Big-O Notation and algorithms","excerpt":"Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm.","path":"/tech/big-o-notation","date":"August 02, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"big-o-notation-post.png","thumbnail":"big-o-notation-post.png","teaser":"big-o-notation-post.png","credit":"NuuNoel","creditlink":"http://www.nuuneoi.com"},"tags":["algorithms"]},"excerpt":"Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst…","timeToRead":3,"html":"<p>Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm.</p>\n<p>A function's Big-O notation is determined by how it responds to different inputs. How much slower is it if we give it a list of 1000 things to work on instead of a list of 1 thing?</p>\n<p>Consider this code:</p>\n<pre><code class=\"language-python\">def item_in_list(to_check, the_list):\n    for item in the_list:\n        if to_check == item:\n          return True\n    return False\n</code></pre>\n<p>If we call this function like <code>item_in_list(2, [1,2,3])</code>, it should be quick. We loop over each thing in the list and if we find the first argument to our function, return True. If we get to the end and we didn't find it, return False.</p>\n<p>The <em>\"complexity\"</em> of this function is <strong>O(n)</strong>. O(n) is read <em>\"Order of N\"</em> because the O function is also known as the Order function. which deals in <em>orders of magnitude</em>.</p>\n<p>\"Orders of magnitude\" is basically tells the difference between classes of numbers. The difference between 1,000 and 10,000 is pretty big (in fact, its the difference between a junker car and a lightly used one). It turns out that in approximation, as long as you're within an order of magnitude, you're pretty close.</p>\n<p>If we were to graph the time it takes to run this function above with different sized inputs (e.g. an array of 1 item, 2 items, 3 items, etc), we'd see that it approximately corresponds to the number of items in the array. This is called a <code>linear graph</code>. This means that the line is basically straight if you were to graph it.</p>\n<p>If, in the code sample above, our item was always the first item in the list, our code would be really fast! This is true, but Big-O is all about the <strong>approximate worst-case performance of doing something</strong>. The worst case for the code above is that the thing we're searching for isn't in the list at all. (Note: The math term for this is \"upper bound\", which means its talking about the mathematic limit of awfulness).</p>\n<p><img src=\"https://justin.abrah.ms/static/images/o_n__plot.png\" alt=\"image\" title=\"Run Time Characteristics of an O(n) function\"></p>\n<blockquote>\n<p>Run Time characteristics of an O(n) function</p>\n</blockquote>\n<p>Consider this next code snippet:</p>\n<pre><code class=\"language-python\">def is_none(item):\n    return item is None\n</code></pre>\n<p>This function is called <code>O(1)</code> which is called <strong>\"constant time\"</strong>. What this means is no matter how big our input is, it always takes the same amount of time to compute things.</p>\n<p><img src=\"https://justin.abrah.ms/static/images/o_1__plot.png\" alt=\"o1_charactersitics\" title=\"Run time characteristics of O(1) function\"></p>\n<blockquote>\n<p>Run time charactersitics of O(1) function</p>\n</blockquote>\n<p>Consider this next example.</p>\n<pre><code class=\"language-python\">def combinations(the_list):\n   results = []\n   for item in the_list:\n       for inner_item in the_list:\n           results.append((item, inner_item))\n   return results\n</code></pre>\n<p>This matches every item in the list with every other item in the list. If we gave it an array <code>[1,2,3]</code>, we'd get back <code>[(1,1) (1,2), (1,3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]</code>. This is part of the field of <strong>combinatorics</strong>, which is the mathematical field which studies combinations of things. This function is considered <strong>O(n^2)</strong>. This is because for every item in the list we have to do n more operations. So n * n == n^2.</p>\n<p>Below is a comparison of each of these graphs, for reference. You can see that an O(n^2) function will get slow very quickly where as something that operates in constant time will be much better. This is particularly useful when it comes to data structures.</p>\n<p><img src=\"https://justin.abrah.ms/static/images/runtime_comparison.png\" alt=\"comparison\"></p>\n<blockquote>\n<p>Comparison of O(n), O(1) and O(n^2) functions</p>\n</blockquote>\n<p>Another Big O notation term is <strong>O(2^N)</strong> denotes an algorithm whose growth doubles with each additon to the input data set. The growth curve of an O(2^N) function is exponential - starting off very shallow, then rising meteorically. An example of an O(2^N) function is the recursive calculation of Fibonacci numbers:</p>\n<p>An example:</p>\n<pre><code class=\"language-python\">def fibonacci(number):\n    if number &#x3C;=1 :\n        return number\n    return fibonacci(number - 2) + fibonacci(number - 1);\n}\n</code></pre>"}},{"node":{"frontmatter":{"title":"Algorithm Analysis","subtitle":"What is algorithm analysis","excerpt":"Algorithms are steps taken to solve a problem. Analysis of an algorithm helps us determine whether the algorithm is useful or not. Once we have a correct algorithm we have to determine whether the algorithm is efficient or not. Effieciency in any running system is important to achieve maximum performance within a given amount of time. Complexity should be at a minimum. Abstraction is key in this process.","path":"/tech/algorithms-analysis","date":"July 18, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"algorithm-analysis.jpg","thumbnail":"algorithm-analysis.jpg","teaser":"algorithm-analysis.jpg","credit":null,"creditlink":null},"tags":["complexity of algorithms","algorithms","algorithm analysis"]},"excerpt":"Algorithms are steps taken to solve a problem. Analysis of an algorithm helps us determine whether the algorithm is useful or not. Once we…","timeToRead":1,"html":"<p>Algorithms are steps taken to solve a problem. Analysis of an algorithm helps us determine whether the algorithm is useful or not. Once we have a correct algorithm we have to determine whether the algorithm is efficient or not. Effieciency in any running system is important to achieve maximum performance within a given amount of time. Complexity should be at a minimum. Abstraction is key in this process.</p>\n<h1>Time Complexity Categories</h1>\n<p>Measures the time taken for an algorithm to execute a problem given a certain input. Categories include:</p>\n<ul>\n<li><strong>Worst Case complexity</strong>: When the amount ot time required by an algorithm for a given input is maximum.</li>\n<li><strong>Average-case complexity</strong>: When the amount of time required by an algorithm for a given input is average.</li>\n<li><strong>Best case complexity</strong>: When the amount of time required by an algorithm for a given input is minimum.</li>\n</ul>\n<h1>Space Complexity</h1>\n<p>Space taken by an algorithm to execute a given problem. This depends on the data structure being worked on</p>\n<h1>Asymptotic Analysis</h1>\n<p>Kind of analysis done to calculate the complexity of an algorithm in its theoretical analysis. A alarge length of input is used to calculate the complexity function of the algorithm. A graph is usually used.\nAsymptotic denotes a condition where a line tends to meet a curve, but they do not intersect. Here the line and the curve is asymptotic to each other. It involves computing the running time of any any operation in mathematical units of computation.\nThe <em>Big-O-notation</em> is used for this</p>\n<h2>Asymptotic notations</h2>\n<ul>\n<li>Big-O notation usually denoted as <strong>O</strong>. Usually used</li>\n<li>Omega</li>\n<li>Theta</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Complexity of Algorithms","subtitle":"Are algorithms that complex?","excerpt":"The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.","path":"/tech/algorithm-complexity","date":"July 17, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"algorithm-complexity.jpg","thumbnail":"algorithm-complexity.jpg","teaser":"algorithm-complexity.jpg","credit":null,"creditlink":null},"tags":["complexity of algorithms","algorithms"]},"excerpt":"The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.\nSo, let's return to some algorithms and see…","timeToRead":6,"html":"<p>The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.\nSo, let's return to some algorithms and see if we learned anything.\nConsider this simple procedure that sums a list (of numbers, we assume):\nprocedure sum(list)\ntotal = 0\nfor i from 0 to length(list)-1\ntotal += list[i]\nreturn total\nFirst: is the algorithm correct? Does it solve the problem specified?\nSecond: is it fast?\nTo evaluate the running time of an algorithm, we will simply ask how many “steps” it takes.\nIn this case, we can count the number of times it runs the += line.\nFor a list with (n) elements, it takes (n) steps.\nOr is counting the += line the right thing to do?\nWhen implementing the for loop, each iteration requires an add (for the loop index) and a comparison (to check the exit condition). We should count those.\nAlso, the variable initialization and return steps.\nSo, (3n+2) steps.\nBut, not all of those steps are the same.\nHow long does an x86 ADD instruction take compared to a CMP or RET instruction?\nWill the compiler keep both i and total in registers, or will one/both be in RAM? (A factor of ~10 difference.)\nHow do those instructions interact in the pipeline? Which can be sent through parallel pipelines in the processor and executed concurrently?\nThe answer to those is simple: I don't know and you don't either.\nThat's part of the reason we're asking about algorithms, not programs.\nBut both (n) and (3n+2) are perfectly reasonable proposals for the answer.\nDeciding between them requires more knowledge about the actual implementation details than we have.\nGood thing we have the function growth notation.\nRemember: this is easy for (n=5) elements. A good or bad algorithm will both be fast then.\nWe want to know how the algorithm behaves for large (n).\nFinally our answer: the sum procedure has running time (\\Theta(n)).\nWe'll say that this algorithm has time complexity (\\Theta(n)), or “runs in linear time”.\nBoth (n) and (3n+2) are (\\Theta(n)), and so is any other “exact” formula we could come up with.\nThe easy answer (count the += line) was just as correct as the very careful one.\nThe big-Θ notation hides all of the details we can't figure out anyway.\nAnother example: print out the sum of each two numbers in a list.\nThat is, given the list [1,2,3,4,5], we want to find 1+2, 1+3, 1+4, 1+5, 2+3, 2+4,….\nPseudocode:\nprocedure sum<em>pairs(list)\nfor i from 0 to length(list)-2\nfor j from i+1 to length(list)-1\nprint list[i] + list[j]\nFor a list with (n) elements, the for j loop iterates (n-1) times when it is called with i==0, then (n-2) times, then (n-3) times,…\nSo, the total number of times the print step runs is [\\begin{align\\</em>} (n-1)+(n-2)+\\cdots+2+1 &#x26;= \\sum<em>{k=1}^{n-1} k\\ &#x26;= \\frac{n(n-1)}{2}\\ &#x26;= \\frac{n^2}{2}-\\frac{n}{2},. \\end{align\\</em>}]\nIf we had counted the initialization of the for loops, counter incrementing, etc, we might have come up with something more like (\\frac{3}{2}n^2 + \\frac{1}{2}n + 1).\nEither way, the answer we give is that it takes (\\Theta(n^2)) steps.\nOr, the algorithm “has time complexity (\\Theta(n^2))” or “has (\\Theta(n^2)) running time” or “has quadratic running time”.\nThe lesson: when counting running time, you can be a bit sloppy.\nWe only need to worry about the inner-most loop(s), not the number of steps in there, or work in the outer levels.\n… because they are going to disappear anyway as constant factors and lower-order terms when they go into a big-O/Ω/Θ anyway.\nAverage and Worst Case</p>\n<p>Consider a linear search: we want to find an element in a list and return its (first) position, or -1 if it's not there.\nprocedure linear_search(list, value)\nfor i from 0 to length(list)-1\nif list[i] == value\nreturn i\nreturn -1\nHow many steps there?\nThe answer is: it depends.\nIf the thing we're looking for is in the first position, it takes (\\Theta(1)) steps.\nIf it's at the end, or not there, it takes (\\Theta(n)) steps.\nThe easiest thing to count is usually the worst case: what is the maximum steps required for any input of size (n)?\nThe worst case is that we go all the way to the end of the list, but don't find it and return -1.\nThe only line that makes sense to count here is the if line. It's in the inner-most loop, and is executed for every iteration.\nWe could also count the number of comparisons made: the == and the implicit comparison in the for loop.\nThat is either (n) or (2n+1) steps, so (\\Theta(n)) complexity.\nThe other useful option is the average case: what is the average steps required over all inputs of size (n)?\nMuch harder to calculate, since you need to consider every possible input to the algorithm.\nEven if we assume the element is found, the possible number of comparisons are:\nFound in position Comparisons\n1 2\n2 4\n⋮ ⋮\n(n) (2n)\nOn average, the number of comparisons is: [\\frac{2+4+\\cdots+2n}{n} = n+1,.]\nAgain, we have (\\Theta(n)) complexity.\n… but it's a good thing we checked. Some algorithms are different.\nGood/bad times</p>\n<p>We have said that these running times are important when it comes to running times of algorithm.\nBut we are throwing away a lot of information when we look only at big-O/Ω/Θ.\nThe lower-order terms must mean something.\nThe leading constants definitely do.\nAssuming one million operations per second, this is the approximate running time of an algorithm given running time, with an input of size (n):\n(n) (\\log<em>2 n) (n) (n\\log</em>2 n) (n^2) (n^{3}) (2^n)\n(10) 3.3 μs 10 μs 33 μs 100 μs 1 ms 1 ms\n(10^2) 6.6 μs 100 μs 664 μs 10 ms 1 s (4\\times 10^{16}) years\n(10^4) 13 μs 10 ms 133 ms 1.7 minutes 11.6 days (10^{2997}) years\n(10^6) 20 μs 1 s 20 s 11.6 days 32000 years (10^{300000}) years\nMaybe that gives a little idea why we'll only worry about complexity\n… at least at first.\nA summary:\nIf you can get (O(\\log n)) life is good: hand it in and go home.\n(O(n\\log n)) is pretty good: hard to complain about it.\n(O(n^k)) could be bad, depending on (k): you won't be solving huge problems. These are polynomial complexity algorithms for (k\\ge 1).\n(\\Omega(k^n)) is a disaster: almost as bad as no algorithm at all if you have double-digit input sizes. These are exponential complexity algorithms for (k\\gt 1).\nSee also: Numbers everyone should know\nA problem that has a polynomial-time algorithm is called tractable.\nNo polynomial time algorithm: intractable.\nThere is a big category of problems that nobody has a polynomial-time algorithm for, but also can't prove that none exists: the NP-complete problems.\nSome examples: Boolean satisfiability, travelling salesman, Hamiltonian path, many scheduling problems, Sudoku (size (n)).\nIf you have an algorithm with a higher complexity than necessary, no amount of clever programming will make up for it.\nNo combination of these will make a (O(n^2)) algorithm faster than an (O(n\\log n)): faster language, better optimizer, hand-optimization of code, faster processor.\nImportant point: the complexity notations only say things about large (n).\nIf you always have small inputs, you might not care.\nAlgorithms with higher complexity class might be faster in practice, if you always have small inputs.\ne.g. Insertion sort has running time (\\Theta(n^2)) but is generally faster than (\\Theta(n\\log n)) sorting algorithms for lists of around 10 or fewer elements.\nThe most important info that the complexity notations throw away is the leading constant.\nThere is a difference between (n^2) instructions and (100n^2) instructions to solve a problem.\nOnce you have the right big-O, then it's time to worry about the constants.\nThat's what clever programming can do.\nWhen we're talking about algorithms (and not programming), the constants don't usually matter much.\nIt's rare to have an algorithm with a big leading constant.\nSo it's not really possible to decide between the algorithms.\nUsually it's a choice between (4n\\log n) or (5n\\log n): you probably have to implement, compile, and profile to decide for sure.\nExample: sorting algorithms. There are several algorithms to sort a list/array.\nInsertion/Selection/Bubble Sorts: (\\Theta(n^2)).\nMerge/Heap Sorts: (\\Theta(n\\log n)).\nQuicksort: (\\Theta(n\\log n)) average case but (very rarely) (\\Theta(n^2)) worst case.\nBut quicksort is usually faster in practice.\n… except when it isn't.\nSeveral recent languages/libraries have implemented a heavily-optimized mergesort (e.g. Python, Perl, Java ≥JDK1.3, Haskell, some STL implementations) instead of Quicksort (C, Ruby, some other STL implementations).\nSpace Complexity</p>\n<p>We have only been talking about running time/speed so far.\nIt also makes good sense to talk about the complexity of other things.\nMost notably, memory use by an algorithm.\nAn algorithm that uses (\\Theta(n^{3})) space is bad. Maybe as bad as (\\Theta(n^{3})) time.\nAn algorithm that uses (O(1)) extra space (in addition to space needed to store the input) is called in-place.\ne.g. selection sort is in-place, but mergesort ((\\Theta(n)) extra space) and Quicksort ((\\Theta(\\log n)) extra space, average case) aren't.</p>"}},{"node":{"frontmatter":{"title":"Sorting Algorithms","subtitle":"Brief overview of some sorting algorithms","excerpt":"Algorithms are set to achieve a certain outcome. Sorting algoritms follow numerical order and usually has random access to an array. This means that it can access random indexes of an array to perform a sort.","path":"/tech/sorting-algorithms","date":"July 12, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"sorting-algorithms.png","thumbnail":"sorting-algorithms.png","teaser":"sorting-algorithms.png","credit":null,"creditlink":null},"tags":["algorithms","sorting algorithms","array"]},"excerpt":"Algorithms are set to achieve a certain outcome. Sorting algoritms follow numerical order and usually has random access to an array. This…","timeToRead":2,"html":"<p>Algorithms are set to achieve a certain outcome. Sorting algoritms follow numerical order and usually has random access to an array. This means that it can access random indexes of an array to perform a sort.</p>\n<h1>Bubble Sort</h1>\n<p>Convinient for small data sets. Consider an array of elements such that:\nint[] arr = new int[]{14,33,27,10,33,19,42,44};\nThe array is not sorted. To sort this array, we compare 2 values at a time.\nSteps:</p>\n<ul>\n<li>compare 14 and 33, since 14 is less than 33, it remains in place</li>\n<li>next we compare 33 and 27, 27 is less than 33, hence we have to swap postions such that;\n<code>arr[1]</code> = 33 takes the place of <code>arr[2]</code> and vice versa.\nThe array now becomes:\n{14,27,33,10,33,19,48,44}</li>\n<li>The next comparison is now between 33 and 10. 10 is less than 33, so we swap positions as before.</li>\n<li>The process continues until all the elements in the array follow natural ordering.</li>\n</ul>\n<p>This is obviously convenient for small data sets as we are comparing 2 elements at a time. If the array was of length 50, it would take much longer to complete and would be inefficient. This is not what an ideal algorithm should achieve.</p>\n<pre><code class=\"language-java\">    /**\n     * @implNote\n     * set flag to true to begin first pass, initialize the step variable, create the temp variable\n     * within while loop, set the flag to false awaiting first pass\n     * record the steps taken in a variable called steps\n     * holds the temp value for the current element in array\n     * assign the current position of the array to the next element\n     * assign the next position of the array to the temp value\n     * set the flag to true to allow continuing of loop, record the steps taken\n     * @param toSort array to perform the bubble sort\n     * @return toSort the sorted array\n     * */\n    public static int[] bubbly(int[] toSort){\n        boolean flag = true;\n        int steps = 0;\n        int temp;\n        while(flag){\n            flag = false;\n            for(int x = 0; x &#x3C; toSort.length-1;x++){\n                if(toSort[x] > toSort[x+1]){\n                    steps++;\n                    temp = toSort[x];\n                    toSort[x] = toSort[x+1];\n                    toSort[x+1] = temp;\n                    flag = true;\n                }\n\n            }\n        }\n        System.out.println(String.valueOf(steps)+ \" steps taken\");\n        return toSort;\n    }\n\n    public static void main(String[] args){\n        int[] arr = new int[]{18,29,1,100,17};\n        int[] arrTwo = new int[]{100,16,33,48,60,21,80};\n        /*Bubble sort Ascending*/\n        System.out.println(Arrays.toString(bubbly(arr)));\n        System.out.println(Arrays.toString(bubbly(arrTwo)));\n        /*5 steps taken\n        [1, 17, 18, 29, 100]\n        9 steps taken\n        [16, 21, 33, 48, 60, 80, 100]*/\n    }\n</code></pre>\n<blockquote>\n<p>Demonstration of a bubble sort algorithm, sorting elements in ascending order</p>\n</blockquote>\n<p>Code snippet right <a href=\"https://github.com/BrianLusina/Java-Playground/blob/master/Toy%20Problems/src/SortingAlgorithms/BubbleSortDemo.java\">here</a>.</p>\n<h1>Merge Sort</h1>\n<p>Uses the <em>divide and conquer rule</em> it divides a problem into smaller parts until it reaches the simplest form possible. Then it rejoins the divided elements in a sorted format until it is a full list again. This sorting algorithm is mostly used in arrays.</p>\n<p>Consider this example\n27 10 12 25 34 16 15 31\ndivide it into two parts\n27 10 12 25 34 16 15 31\ndivide each part into two parts\n27 10 12 25 34 16 15 31\ndivide each part into two parts\n27 10 12 25 34 16 15 31</p>\n<p>merge (cleverly-!) parts</p>\n<p>10 27 12 25 16 34 15 31\nmerge parts\n10 12 25 27 15 16 31 34\nmerge parts into one\n10 12 15 16 25 27 31 34</p>\n<h1>Insertion Sort</h1>"}}]}},"pageContext":{"tag":"algorithms"}}}