{"componentChunkName":"component---src-templates-blogs-blog-post-jsx","path":"/design/test-users-usabilty-studies","result":{"data":{"markdownRemark":{"html":"<p>Following my <a href=\"https://brianlusina.github.io/Paper-Rabbit/articles/usability-and-code/\">previous</a> article, I think it best to write about how many users are the optimal number to conduct a usability test with. Most would argue that you need hundreds to come up with an optimal number, say, 100? However, this would be wrong, very wrong. In fact, based on my own personal research the number is 5, except when it is not. This means the optimal number is 5, but then again depends on a couple of factors.</p>\n<p>Why 5 though? This lets you find almost as many usability problems as you'd find using many more test participants. With 5 users, you almost always get close to user testing's maximum benefit-cost ratio.</p>\n<p>There are exceptions to the rule of course.</p>\n<ul>\n<li><strong>Quantitative studies aim at statistics more than on insights</strong>. In this case test at least 20 users to get statistically significant numbers; tight confidence intervals require even more users.</li>\n<li><strong>Card Sorting</strong> test at least 15 users.</li>\n<li><strong>Eyetracking</strong> Test 39 users if you want stable heatmaps.</li>\n</ul>\n<p>These exceptions shouldn't worry you much. The vast majority of your user research should be qualitative, aimed at collecting insights to drive your design, not numbers to impress people in PowerPoint.</p>\n<p>The main argument for small tests is simply <strong>return on investment</strong>. Testing costs increase with each additional study participant, yet the number of findings quickly reaches the point of diminishing returns. There's little additional benefit to running more than 5 people through the same study; ROI drops like a stone with a bigger number.</p>\n<p>And if you have a big budget? Spend it on additional studies, not more users in each study.</p>\n<h2>Arguments for more Test participants</h2>\n<ol>\n<li><strong>A big website has tons of users.</strong> Doesn't matter for the sample size, even if you were doing statistics. The variance in statistical sampling is determined by the sample size, not the size of the full population from which the sample was drawn. In user testing, we focus on a website's functionality to see which design elements are easy or difficult to use. The evaluation of a design element's quality is independent of how many people use it. (Conversely, the decision about whether to fix a design flaw should certainly consider how much use it'll get: it might not be worth the effort to improve a feature that has few users; better to spend the effort recoding something with millions of users.).</li>\n<li><strong>A big website has tons of features</strong> This is an argument for running several different tests — each focusing on a smaller set of features — not for having more users in each test. You can't ask any individual to test more than a handful of tasks before the poor user is tired out. Yes, you'll need more users overall for a feature-rich design, but you need to spread these users across many studies, each focusing on a subset of your research agenda.</li>\n<li>\n<p><strong>We have several different target audiences.</strong> This can actually be a legitimate reason for testing a larger user set because you'll need representatives of each target group. However, this argument holds <strong>only if</strong> the different users are actually going to behave in completely different ways. Some examples of projects may include:</p>\n<ul>\n<li>Medical site targeting both doctors and patients, and</li>\n<li>Auction site where you can either sell stuff or buy stuff.</li>\n</ul>\n<p>When the users and their tasks are this different, you're essentially running a new test for each target audience, and you'll need close to 5 users per group. Typically, you can get away with 3–4 users per group because the user experience will overlap somewhat between the two groups. With, say, a financial site that targets novice, intermediate, and experienced investors, you might test 3 of each, for a total of 9 users — you won't need 15 users total to assess the site's usability.</p>\n</li>\n<li>\n<p><strong>The site makes so much money that even the smallest usability problem is unacceptable.</strong> Rich companies certainly have an ROI case to spend more on usability. Even if they spend \"too much\" on each quality improvement, they'll make even more back because of the vast amounts of money flowing through the user interface. However, even the highest-value design projects will still optimize their ROI by keeping each study small and conducting many more studies than a lower-value project could afford.</p>\n<p>The basic point is that it's okay to leave usability problems behind in any one version of the design as long as you're employing an iterative design process where you'll design and test additional versions. Anything not fixed now will be fixed next time. If you have many things to fix, simply plan for a lot of iterations. The end result will be higher quality (and thus higher business value) due to the additional iterations than from testing more users each time.</p>\n</li>\n</ol>\n<p>True answer to \"how many users\" can sometimes be much smaller than 5. If you have an Agile-style UX process with very low overhead, your investment in each study is so trivial that the cost–benefit ratio is optimized by a smaller benefit. (It might seem counterintuitive to end up with more money by making less money from each study, but this occurs because the smaller overhead lets you run so many more studies that the sum of numerous small benefits becomes a big number.)</p>\n<p>For really low-overhead projects, it's often optimal to test as little as 2 users per study. For some other projects, 8 users — or sometimes even more — might be better. For most projects, however, you should stay with the tried-and-true: 5 users per usability test.</p>\n<h2>Conclusion</h2>\n<p>Performing these usability studies is necessary to determing usability of your software. However, one should note that sometimes less is more.</p>","frontmatter":{"title":"Test Users and Usability","subtitle":"How many users do you need to test usability of your software?","date":"August, 04, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"test-users-and-usability.jpg","thumbnail":"test-users-and-usability.jpg","teaser":"test-users-and-usability.jpg","credit":null,"creditlink":null},"path":"/design/test-users-usabilty-studies","tags":["Usability","User Experience","UX","Design"],"excerpt":"How many users do you need to test usability of your software?"}}},"pageContext":{"prev":{"html":"<p>NumPy is <em>Numerical Python</em> in full. A very powerful library for performing, well you guessed it, vector arithmetic. For those who shall continue on with Data Science using Python, I suggest you have a look at the NumPy package. It is a very powerful tool that will most definitely make your life that much simpler.</p>\n<p>A basic example of the use of NumPy</p>\n<pre><code class=\"language-python\">from numpy import array\n\nbaseball = [12, 46, 489, 46, 5, 312, 31, 2, 3, 12, 31, 3, 13, 1, 31, 3, 13, 13, 1, 31, 313, 1, 31, 189, 4, 4, 31, 564,\n            9, 19, 416, 49, 498, 4984, 1984]\n\nprint(type(array(baseball)))\n&#x3C;class 'numpy.ndarray'>\n\nprint(type(baseball))\n&#x3C;class 'list'>\n</code></pre>\n<blockquote>\n<p>The 2 types are not similar, but almost the same operations can be performed on them.</p>\n</blockquote>\n<h2>Performing simple operations</h2>\n<p>Say for example you get the heights of each football player in England. You call the Barclay's Premier League and since you are a major fan and a major stakeholder in Barclays(They don't just send this data to everyone, I tried), they send you the data of 1000 players' heights as a list. Sadly, they are all in inches and you do not use inches, but rather meters. You could use a <code>for loop</code> converting each to meters and multiplying by 0.0254. This will work, but it will take quite a while to execute.</p>\n<pre><code class=\"language-python\">heights = [78,45,70,80,75,84,76,84,71,72,73,84,75,88,79,...]\nheights_m = [x * 0.0254 for x in heights]\nprint(heights_m)\n</code></pre>\n<blockquote>\n<p>This uses a list comprehension to obtain a new list of the newly converted heights to meters.</p>\n</blockquote>\n<p>NumPy on the other hand makes this process very simple and efficient. You will not have to use a for loop to perform such an operation.</p>\n<pre><code class=\"language-python\">from numpy import array\nheights = [78,45,70,80,75,84,76,84,71,72,73,84,75,88,79,...]\n\nheights_m = array(heights) * 0.0254\nprint(m)\n</code></pre>\n<blockquote>\n<p>The output will be the same. NumPy array function simply took in the list of heights and multiplied each to 0.0254. No loop used.</p>\n</blockquote>\n<p>The same could apply for weights. Say we now get weight data from the BPL. Unfortunately, again, they send data in pounds. We don't use pounds, instead we decide to use <em>kilograms</em>. we could do the same for loop above instead replace the <code>heights</code> with weights and instead use <code>0.453592</code> to perform the conversion. Alternatively, you guessed it, we could use <code>array</code> function from numpy to perform the operation such that it becomes <code>array(weights) * 0.453592</code> which makes our life that much easier.</p>\n<p>Maybe you will say. 'So what, the results are the same, so why use NumPy?'. True, they are the same, so the used of NumPy seems a little too much.</p>\n<p>This is why.</p>\n<p>Say, now you want to calculate the <strong>Body Mass Index</strong> (BMI) of each player in the BPL, well anonymously, considering the BPL did not send names attached to the data. This would require you to perform a loop within a loop. This is because to calculate the BMI we use this formula:</p>\n<pre><code class=\"language-plain\">BMI = weight (in kilograms)\n       height (in meters)^2\n</code></pre>\n<p>Now, imagine performing a for loop in the two lists we now have <code>weights_kg</code> and <code>heights_m</code>. If you were to use standart Python this would be the most probable way to perform the operation:</p>\n<pre><code class=\"language-python\">bmi = []\nm = list(zip(weights_kg, heights_m))\nfor x, y in m:\n    bmi.append(x/y**2)\n\nprint(bmi)\n</code></pre>\n<blockquote>\n<p><code>zip</code> function creats an iterator that aggregates elements from each of the iterables.</p>\n</blockquote>\n<p>With NumPy you can easily do the same with fewer lines and with less of a headache:</p>\n<pre><code class=\"language-python\">bmi = weights_m / heights_m **2\nprint(bmi)\n</code></pre>\n<blockquote>\n<p>This is after the weights and heights lists have been passed as arguments to the <code>array</code> function of the NumPy module.</p>\n</blockquote>\n<p>The results are the same, but it is more intuitive with NumPy, you can not perform the same operation with standard Python. So doint this : <code>bmi = weights_m / heights_m **2</code> without passing the <code>weights_m</code> and <code>heights_kg</code> as arguments in NumPy's array function will cause an error. Go ahead and try that :).</p>\n<h2>Subsetting</h2>\n<p>Lists in Python can be <em>subsetted</em>, if that is a word at all. By subsetting a Python list this is what I mean:</p>\n<pre><code class=\"language-python\">x = [4 , 9 , 6, 3, 1]\nx[1]\nimport numpy as np\ny = np.array(x)\ny[1]\n</code></pre>\n<blockquote>\n<p>This is subsetting using squre brackets, this applies to both NumPy and standard Python lists</p>\n</blockquote>\n<p>But NumPy has something special about subsetting that the standard Python lists do not have.\nFor Numpy specifically, you can also use boolean Numpy arrays:</p>\n<pre><code class=\"language-python\">high = y > 5\ny[high]\n</code></pre>\n<p>Now, to put it all together:</p>\n<pre><code class=\"language-python\"># height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Calculate the BMI: bmi\nnp_height_m = np.array(height) * 0.0254\nnp_weight_kg = np.array(weight) * 0.453592\nbmi = np_weight_kg / np_height_m ** 2\n\n# Create the light array\nlight = np.array(bmi) &#x3C; 21\n\n# Print out light\nprint(light)\n\n# Print out BMIs of all baseball players whose BMI is below 21\nprint(bmi[light])\n</code></pre>\n<p>Numpy is great to do vector arithmetic. If you compare its functionality with regular Python lists, however, some things have changed.</p>\n<p>First of all, Numpy arrays cannot contain elements with different types. If you try to build such a list, some of the elments' types are changed to end up with a homogenous list. This is known as type coercion.</p>\n<p>Second, the typical arithmetic operators, such as +, -, * and / have a different meaning for regular Python lists and Numpy arrays.</p>\n<p>Have a look at this line of code:</p>\n<pre><code class=\"language-python\">>>>np.array([True, 1, 2]) + np.array([3, 4, False])\narray([4, 5, 2])\n</code></pre>\n<p>Python lists and Numpy arrays sometimes behave differently. Luckily, there are still certainties in this world. For example, subsetting (using the square bracket notation on lists or arrays) works exactly the same. To see this for yourself, try the following lines of code in the IPython Shell:</p>\n<pre><code class=\"language-python\">x = [\"a\", \"b\", \"c\"]\nx[1]\n\nnp_x = np.array(x)\nnp_x[1]\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Store weight and height lists as numpy arrays\nnp_weight = np.array(weight)\nnp_height = np.array(height)\n\n# Print out the weight at index 50\nprint(np_weight[50])\n\n# Print out sub-array of np_height: index 100 up to and including index 11077\nprint(np_height[100:111])\n</code></pre>\n<h2>Conclusion</h2>\n<p>Of course there is more to the NumPy module that I have not covered, this was and is to cite that the NumPy array and the Python Lists are the same but you can perform certain operations on NumPy arrays that you can not perform on Python lists. NumPy is a powerful library to uses especially if you will become a data scientist of use if for <strong>Big Data</strong>.</p>","id":"8d3c99f8-70ac-5fb9-b281-8b1d1e53a96f","timeToRead":5,"frontmatter":{"title":"NumPy","subtitle":"NumPy Arrays and Python Lists","excerpt":"NumPy is _Numerical Python_ in full. A very powerful library for performing, well you guessed it, vector arithmetic. For those who shall continue on with Data Science using Python, I suggest you have a look at the NumPy package. It is a very powerful tool that will most definitely make your life that much simpler.","path":"/tech/numpy","category":"tech","date":"September 03, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"numpy.jpg","thumbnail":"numpy.jpg","teaser":"numpy.jpg","credit":"NumPy","creditlink":"http://www.numpy.org/"},"tags":["Python","Numpy","data"],"published":true}},"next":{"html":"<p>Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm.</p>\n<p>A function's Big-O notation is determined by how it responds to different inputs. How much slower is it if we give it a list of 1000 things to work on instead of a list of 1 thing?</p>\n<p>Consider this code:</p>\n<pre><code class=\"language-python\">def item_in_list(to_check, the_list):\n    for item in the_list:\n        if to_check == item:\n          return True\n    return False\n</code></pre>\n<p>If we call this function like <code>item_in_list(2, [1,2,3])</code>, it should be quick. We loop over each thing in the list and if we find the first argument to our function, return True. If we get to the end and we didn't find it, return False.</p>\n<p>The <em>\"complexity\"</em> of this function is <strong>O(n)</strong>. O(n) is read <em>\"Order of N\"</em> because the O function is also known as the Order function. which deals in <em>orders of magnitude</em>.</p>\n<p>\"Orders of magnitude\" is basically tells the difference between classes of numbers. The difference between 1,000 and 10,000 is pretty big (in fact, its the difference between a junker car and a lightly used one). It turns out that in approximation, as long as you're within an order of magnitude, you're pretty close.</p>\n<p>If we were to graph the time it takes to run this function above with different sized inputs (e.g. an array of 1 item, 2 items, 3 items, etc), we'd see that it approximately corresponds to the number of items in the array. This is called a <code>linear graph</code>. This means that the line is basically straight if you were to graph it.</p>\n<p>If, in the code sample above, our item was always the first item in the list, our code would be really fast! This is true, but Big-O is all about the <strong>approximate worst-case performance of doing something</strong>. The worst case for the code above is that the thing we're searching for isn't in the list at all. (Note: The math term for this is \"upper bound\", which means its talking about the mathematic limit of awfulness).</p>\n<p><img src=\"https://justin.abrah.ms/static/images/o_n__plot.png\" alt=\"image\" title=\"Run Time Characteristics of an O(n) function\"></p>\n<blockquote>\n<p>Run Time characteristics of an O(n) function</p>\n</blockquote>\n<p>Consider this next code snippet:</p>\n<pre><code class=\"language-python\">def is_none(item):\n    return item is None\n</code></pre>\n<p>This function is called <code>O(1)</code> which is called <strong>\"constant time\"</strong>. What this means is no matter how big our input is, it always takes the same amount of time to compute things.</p>\n<p><img src=\"https://justin.abrah.ms/static/images/o_1__plot.png\" alt=\"o1_charactersitics\" title=\"Run time characteristics of O(1) function\"></p>\n<blockquote>\n<p>Run time charactersitics of O(1) function</p>\n</blockquote>\n<p>Consider this next example.</p>\n<pre><code class=\"language-python\">def combinations(the_list):\n   results = []\n   for item in the_list:\n       for inner_item in the_list:\n           results.append((item, inner_item))\n   return results\n</code></pre>\n<p>This matches every item in the list with every other item in the list. If we gave it an array <code>[1,2,3]</code>, we'd get back <code>[(1,1) (1,2), (1,3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]</code>. This is part of the field of <strong>combinatorics</strong>, which is the mathematical field which studies combinations of things. This function is considered <strong>O(n^2)</strong>. This is because for every item in the list we have to do n more operations. So n * n == n^2.</p>\n<p>Below is a comparison of each of these graphs, for reference. You can see that an O(n^2) function will get slow very quickly where as something that operates in constant time will be much better. This is particularly useful when it comes to data structures.</p>\n<p><img src=\"https://justin.abrah.ms/static/images/runtime_comparison.png\" alt=\"comparison\"></p>\n<blockquote>\n<p>Comparison of O(n), O(1) and O(n^2) functions</p>\n</blockquote>\n<p>Another Big O notation term is <strong>O(2^N)</strong> denotes an algorithm whose growth doubles with each additon to the input data set. The growth curve of an O(2^N) function is exponential - starting off very shallow, then rising meteorically. An example of an O(2^N) function is the recursive calculation of Fibonacci numbers:</p>\n<p>An example:</p>\n<pre><code class=\"language-python\">def fibonacci(number):\n    if number &#x3C;=1 :\n        return number\n    return fibonacci(number - 2) + fibonacci(number - 1);\n}\n</code></pre>","id":"7c19ef19-bd32-5ef7-822b-bddde609db81","timeToRead":3,"frontmatter":{"title":"Big-O-Notation","subtitle":"The Big Deal with the Big-O Notation and algorithms","excerpt":"Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm.","path":"/tech/big-o-notation","category":"tech","date":"August 02, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"big-o-notation-post.png","thumbnail":"big-o-notation-post.png","teaser":"big-o-notation-post.png","credit":"NuuNoel","creditlink":"http://www.nuuneoi.com"},"tags":["algorithms"],"published":true}}}}}