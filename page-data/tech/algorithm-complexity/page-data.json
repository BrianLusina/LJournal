{"componentChunkName":"component---src-templates-blogs-blog-post-jsx","path":"/tech/algorithm-complexity","result":{"data":{"markdownRemark":{"html":"<p>The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.\nSo, let's return to some algorithms and see if we learned anything.\nConsider this simple procedure that sums a list (of numbers, we assume):\nprocedure sum(list)\ntotal = 0\nfor i from 0 to length(list)-1\ntotal += list[i]\nreturn total\nFirst: is the algorithm correct? Does it solve the problem specified?\nSecond: is it fast?\nTo evaluate the running time of an algorithm, we will simply ask how many “steps” it takes.\nIn this case, we can count the number of times it runs the += line.\nFor a list with (n) elements, it takes (n) steps.\nOr is counting the += line the right thing to do?\nWhen implementing the for loop, each iteration requires an add (for the loop index) and a comparison (to check the exit condition). We should count those.\nAlso, the variable initialization and return steps.\nSo, (3n+2) steps.\nBut, not all of those steps are the same.\nHow long does an x86 ADD instruction take compared to a CMP or RET instruction?\nWill the compiler keep both i and total in registers, or will one/both be in RAM? (A factor of ~10 difference.)\nHow do those instructions interact in the pipeline? Which can be sent through parallel pipelines in the processor and executed concurrently?\nThe answer to those is simple: I don't know and you don't either.\nThat's part of the reason we're asking about algorithms, not programs.\nBut both (n) and (3n+2) are perfectly reasonable proposals for the answer.\nDeciding between them requires more knowledge about the actual implementation details than we have.\nGood thing we have the function growth notation.\nRemember: this is easy for (n=5) elements. A good or bad algorithm will both be fast then.\nWe want to know how the algorithm behaves for large (n).\nFinally our answer: the sum procedure has running time (\\Theta(n)).\nWe'll say that this algorithm has time complexity (\\Theta(n)), or “runs in linear time”.\nBoth (n) and (3n+2) are (\\Theta(n)), and so is any other “exact” formula we could come up with.\nThe easy answer (count the += line) was just as correct as the very careful one.\nThe big-Θ notation hides all of the details we can't figure out anyway.\nAnother example: print out the sum of each two numbers in a list.\nThat is, given the list [1,2,3,4,5], we want to find 1+2, 1+3, 1+4, 1+5, 2+3, 2+4,….\nPseudocode:\nprocedure sum<em>pairs(list)\nfor i from 0 to length(list)-2\nfor j from i+1 to length(list)-1\nprint list[i] + list[j]\nFor a list with (n) elements, the for j loop iterates (n-1) times when it is called with i==0, then (n-2) times, then (n-3) times,…\nSo, the total number of times the print step runs is [\\begin{align\\</em>} (n-1)+(n-2)+\\cdots+2+1 &#x26;= \\sum<em>{k=1}^{n-1} k\\ &#x26;= \\frac{n(n-1)}{2}\\ &#x26;= \\frac{n^2}{2}-\\frac{n}{2},. \\end{align\\</em>}]\nIf we had counted the initialization of the for loops, counter incrementing, etc, we might have come up with something more like (\\frac{3}{2}n^2 + \\frac{1}{2}n + 1).\nEither way, the answer we give is that it takes (\\Theta(n^2)) steps.\nOr, the algorithm “has time complexity (\\Theta(n^2))” or “has (\\Theta(n^2)) running time” or “has quadratic running time”.\nThe lesson: when counting running time, you can be a bit sloppy.\nWe only need to worry about the inner-most loop(s), not the number of steps in there, or work in the outer levels.\n… because they are going to disappear anyway as constant factors and lower-order terms when they go into a big-O/Ω/Θ anyway.\nAverage and Worst Case</p>\n<p>Consider a linear search: we want to find an element in a list and return its (first) position, or -1 if it's not there.\nprocedure linear_search(list, value)\nfor i from 0 to length(list)-1\nif list[i] == value\nreturn i\nreturn -1\nHow many steps there?\nThe answer is: it depends.\nIf the thing we're looking for is in the first position, it takes (\\Theta(1)) steps.\nIf it's at the end, or not there, it takes (\\Theta(n)) steps.\nThe easiest thing to count is usually the worst case: what is the maximum steps required for any input of size (n)?\nThe worst case is that we go all the way to the end of the list, but don't find it and return -1.\nThe only line that makes sense to count here is the if line. It's in the inner-most loop, and is executed for every iteration.\nWe could also count the number of comparisons made: the == and the implicit comparison in the for loop.\nThat is either (n) or (2n+1) steps, so (\\Theta(n)) complexity.\nThe other useful option is the average case: what is the average steps required over all inputs of size (n)?\nMuch harder to calculate, since you need to consider every possible input to the algorithm.\nEven if we assume the element is found, the possible number of comparisons are:\nFound in position Comparisons\n1 2\n2 4\n⋮ ⋮\n(n) (2n)\nOn average, the number of comparisons is: [\\frac{2+4+\\cdots+2n}{n} = n+1,.]\nAgain, we have (\\Theta(n)) complexity.\n… but it's a good thing we checked. Some algorithms are different.\nGood/bad times</p>\n<p>We have said that these running times are important when it comes to running times of algorithm.\nBut we are throwing away a lot of information when we look only at big-O/Ω/Θ.\nThe lower-order terms must mean something.\nThe leading constants definitely do.\nAssuming one million operations per second, this is the approximate running time of an algorithm given running time, with an input of size (n):\n(n) (\\log<em>2 n) (n) (n\\log</em>2 n) (n^2) (n^{3}) (2^n)\n(10) 3.3 μs 10 μs 33 μs 100 μs 1 ms 1 ms\n(10^2) 6.6 μs 100 μs 664 μs 10 ms 1 s (4\\times 10^{16}) years\n(10^4) 13 μs 10 ms 133 ms 1.7 minutes 11.6 days (10^{2997}) years\n(10^6) 20 μs 1 s 20 s 11.6 days 32000 years (10^{300000}) years\nMaybe that gives a little idea why we'll only worry about complexity\n… at least at first.\nA summary:\nIf you can get (O(\\log n)) life is good: hand it in and go home.\n(O(n\\log n)) is pretty good: hard to complain about it.\n(O(n^k)) could be bad, depending on (k): you won't be solving huge problems. These are polynomial complexity algorithms for (k\\ge 1).\n(\\Omega(k^n)) is a disaster: almost as bad as no algorithm at all if you have double-digit input sizes. These are exponential complexity algorithms for (k\\gt 1).\nSee also: Numbers everyone should know\nA problem that has a polynomial-time algorithm is called tractable.\nNo polynomial time algorithm: intractable.\nThere is a big category of problems that nobody has a polynomial-time algorithm for, but also can't prove that none exists: the NP-complete problems.\nSome examples: Boolean satisfiability, travelling salesman, Hamiltonian path, many scheduling problems, Sudoku (size (n)).\nIf you have an algorithm with a higher complexity than necessary, no amount of clever programming will make up for it.\nNo combination of these will make a (O(n^2)) algorithm faster than an (O(n\\log n)): faster language, better optimizer, hand-optimization of code, faster processor.\nImportant point: the complexity notations only say things about large (n).\nIf you always have small inputs, you might not care.\nAlgorithms with higher complexity class might be faster in practice, if you always have small inputs.\ne.g. Insertion sort has running time (\\Theta(n^2)) but is generally faster than (\\Theta(n\\log n)) sorting algorithms for lists of around 10 or fewer elements.\nThe most important info that the complexity notations throw away is the leading constant.\nThere is a difference between (n^2) instructions and (100n^2) instructions to solve a problem.\nOnce you have the right big-O, then it's time to worry about the constants.\nThat's what clever programming can do.\nWhen we're talking about algorithms (and not programming), the constants don't usually matter much.\nIt's rare to have an algorithm with a big leading constant.\nSo it's not really possible to decide between the algorithms.\nUsually it's a choice between (4n\\log n) or (5n\\log n): you probably have to implement, compile, and profile to decide for sure.\nExample: sorting algorithms. There are several algorithms to sort a list/array.\nInsertion/Selection/Bubble Sorts: (\\Theta(n^2)).\nMerge/Heap Sorts: (\\Theta(n\\log n)).\nQuicksort: (\\Theta(n\\log n)) average case but (very rarely) (\\Theta(n^2)) worst case.\nBut quicksort is usually faster in practice.\n… except when it isn't.\nSeveral recent languages/libraries have implemented a heavily-optimized mergesort (e.g. Python, Perl, Java ≥JDK1.3, Haskell, some STL implementations) instead of Quicksort (C, Ruby, some other STL implementations).\nSpace Complexity</p>\n<p>We have only been talking about running time/speed so far.\nIt also makes good sense to talk about the complexity of other things.\nMost notably, memory use by an algorithm.\nAn algorithm that uses (\\Theta(n^{3})) space is bad. Maybe as bad as (\\Theta(n^{3})) time.\nAn algorithm that uses (O(1)) extra space (in addition to space needed to store the input) is called in-place.\ne.g. selection sort is in-place, but mergesort ((\\Theta(n)) extra space) and Quicksort ((\\Theta(\\log n)) extra space, average case) aren't.</p>","frontmatter":{"title":"Complexity of Algorithms","subtitle":"Are algorithms that complex?","date":"July, 17, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"algorithm-complexity.jpg","thumbnail":"algorithm-complexity.jpg","teaser":"algorithm-complexity.jpg","credit":null,"creditlink":null},"path":"/tech/algorithm-complexity","tags":["complexity of algorithms","algorithms"],"excerpt":"The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms."}}},"pageContext":{"prev":{"html":"<p>Algorithms are steps taken to solve a problem. Analysis of an algorithm helps us determine whether the algorithm is useful or not. Once we have a correct algorithm we have to determine whether the algorithm is efficient or not. Effieciency in any running system is important to achieve maximum performance within a given amount of time. Complexity should be at a minimum. Abstraction is key in this process.</p>\n<h1>Time Complexity Categories</h1>\n<p>Measures the time taken for an algorithm to execute a problem given a certain input. Categories include:</p>\n<ul>\n<li><strong>Worst Case complexity</strong>: When the amount ot time required by an algorithm for a given input is maximum.</li>\n<li><strong>Average-case complexity</strong>: When the amount of time required by an algorithm for a given input is average.</li>\n<li><strong>Best case complexity</strong>: When the amount of time required by an algorithm for a given input is minimum.</li>\n</ul>\n<h1>Space Complexity</h1>\n<p>Space taken by an algorithm to execute a given problem. This depends on the data structure being worked on</p>\n<h1>Asymptotic Analysis</h1>\n<p>Kind of analysis done to calculate the complexity of an algorithm in its theoretical analysis. A alarge length of input is used to calculate the complexity function of the algorithm. A graph is usually used.\nAsymptotic denotes a condition where a line tends to meet a curve, but they do not intersect. Here the line and the curve is asymptotic to each other. It involves computing the running time of any any operation in mathematical units of computation.\nThe <em>Big-O-notation</em> is used for this</p>\n<h2>Asymptotic notations</h2>\n<ul>\n<li>Big-O notation usually denoted as <strong>O</strong>. Usually used</li>\n<li>Omega</li>\n<li>Theta</li>\n</ul>","id":"83ba908b-fcfc-5588-839f-acd97181f031","timeToRead":1,"frontmatter":{"title":"Algorithm Analysis","subtitle":"What is algorithm analysis","excerpt":"Algorithms are steps taken to solve a problem. Analysis of an algorithm helps us determine whether the algorithm is useful or not. Once we have a correct algorithm we have to determine whether the algorithm is efficient or not. Effieciency in any running system is important to achieve maximum performance within a given amount of time. Complexity should be at a minimum. Abstraction is key in this process.","path":"/tech/algorithms-analysis","category":"tech","date":"July 18, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"algorithm-analysis.jpg","thumbnail":"algorithm-analysis.jpg","teaser":"algorithm-analysis.jpg","credit":null,"creditlink":null},"tags":["complexity of algorithms","algorithms","algorithm analysis"],"published":true}},"next":{"html":"<p>There are several Abstract data types that are used in Java,</p>\n<h2>Stacks</h2>\n<p>The Stack class represents a last-in-first-out (LIFO) stack of objects, The last element in will be the first one out, which is unfair if you ask me :smile:. It extends class <a href=\"https://docs.oracle.com/javase/7/docs/api/java/util/Vector.html\"><strong>Vector</strong></a> with five operations that allow a vector to be treated as a stack.</p>\n<p>The usual push and pop operations are provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top.</p>\n<p>When a stack is first created, it contains no items.</p>\n<ul>\n<li><strong>push</strong> adds an item to the top of the stack. This will always be the last one in, but the first one out.</li>\n<li><strong>pop</strong> removes an element from the top of the stack and returns it.</li>\n<li><strong>peek</strong> look at the object at the top of the stack without removing it</li>\n<li><strong>empty</strong> tests if the stack is empty</li>\n<li><strong>search</strong> if the object being sought is in the stack, it returns the 1-based position of the object, which is the distance from the top.</li>\n</ul>\n<p>A practical example, trays piled on top of each other, books laid on top of each other, plates laid on top of each other.</p>\n<p>There is no scenerio in which the stack will remain empty. There will always be data flowing in and data flowing out.</p>\n<p>A code snippet:</p>\n<pre><code class=\"language-java\">public class StacksDemo {\n    public static void main(String[] args){\n        Stack stack = new Stack();\n        System.out.println(\"Stack is \" + stack);\n        showPush(stack,52);\n        showPop(stack);\n        /*output:\n        *Stack is []\n        * push 52\n        * Stack is now: [52]\n        * Popped element is 52\n        * Stack is: []\n        * */\n    }\n\n    /**Method showing how to push elements into a stack*/\n    public static void showPush(Stack stack, int a){\n        stack.push(new Integer(a));\n        System.out.println(\"push \" + String.valueOf(a));\n        System.out.println(\"Stack is now: \"+ stack );\n    }\n\n    /**Method demonstrating the use of pop*/\n    public static void showPop(Stack stack){\n        Integer integer = (Integer) stack.pop();\n        System.out.println(\"Popped element is \" + String.valueOf(integer));\n        System.out.println(\"Stack is: \"+ stack);\n    }\n/*Class end*/\n}\n</code></pre>\n<blockquote>\n<p>code snippet can be found <a href=\"https://github.com/BrianLusina/Java-Playground/blob/master/Toy%20Problems/src/DataStructures/DataTypes/StacksDemo.java\">here</a></p>\n</blockquote>\n<h2>Queue</h2>\n<p>Resembles a normal queue on a bus station, the first in line will be the first one to board the bus, the last will obviously be the last. It is based on a first come first serve\nHas 2 major operations:</p>\n<ul>\n<li>Dequeue</li>\n<li>-</li>\n</ul>\n<h2>LinkedLists</h2>\n<p>Data arranged in a list with the first element being the <strong>head</strong> having the node and a pointer. The pointer points to another node and so on and so forth until it reaches the last element with a node that is null. The last element is known as the <strong>tail</strong>\nThe node consists of where the current data is held and where the next data is held.</p>","id":"dbcc8de6-0a00-59d3-8023-fda0563bdd20","timeToRead":2,"frontmatter":{"title":"Abstract Data Types","subtitle":"All about abstract data types","excerpt":"There are several Abstract data types that are used in Java, this is a brief overview of some of them","path":"/tech/abstract-data-types","category":"tech","date":"July 12, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"abstract-data-type.jpg","thumbnail":"abstract-data-type.jpg","teaser":"abstract-data-type.jpg","credit":null,"creditlink":null},"tags":["data","arrays","lists","stacks","queues"],"published":true}}}}}