{"componentChunkName":"component---src-templates-blogs-blog-post-jsx","path":"/tech/abstract-data-types","result":{"data":{"markdownRemark":{"html":"<p>There are several Abstract data types that are used in Java,</p>\n<h2>Stacks</h2>\n<p>The Stack class represents a last-in-first-out (LIFO) stack of objects, The last element in will be the first one out, which is unfair if you ask me :smile:. It extends class <a href=\"https://docs.oracle.com/javase/7/docs/api/java/util/Vector.html\"><strong>Vector</strong></a> with five operations that allow a vector to be treated as a stack.</p>\n<p>The usual push and pop operations are provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top.</p>\n<p>When a stack is first created, it contains no items.</p>\n<ul>\n<li><strong>push</strong> adds an item to the top of the stack. This will always be the last one in, but the first one out.</li>\n<li><strong>pop</strong> removes an element from the top of the stack and returns it.</li>\n<li><strong>peek</strong> look at the object at the top of the stack without removing it</li>\n<li><strong>empty</strong> tests if the stack is empty</li>\n<li><strong>search</strong> if the object being sought is in the stack, it returns the 1-based position of the object, which is the distance from the top.</li>\n</ul>\n<p>A practical example, trays piled on top of each other, books laid on top of each other, plates laid on top of each other.</p>\n<p>There is no scenerio in which the stack will remain empty. There will always be data flowing in and data flowing out.</p>\n<p>A code snippet:</p>\n<pre><code class=\"language-java\">public class StacksDemo {\n    public static void main(String[] args){\n        Stack stack = new Stack();\n        System.out.println(\"Stack is \" + stack);\n        showPush(stack,52);\n        showPop(stack);\n        /*output:\n        *Stack is []\n        * push 52\n        * Stack is now: [52]\n        * Popped element is 52\n        * Stack is: []\n        * */\n    }\n\n    /**Method showing how to push elements into a stack*/\n    public static void showPush(Stack stack, int a){\n        stack.push(new Integer(a));\n        System.out.println(\"push \" + String.valueOf(a));\n        System.out.println(\"Stack is now: \"+ stack );\n    }\n\n    /**Method demonstrating the use of pop*/\n    public static void showPop(Stack stack){\n        Integer integer = (Integer) stack.pop();\n        System.out.println(\"Popped element is \" + String.valueOf(integer));\n        System.out.println(\"Stack is: \"+ stack);\n    }\n/*Class end*/\n}\n</code></pre>\n<blockquote>\n<p>code snippet can be found <a href=\"https://github.com/BrianLusina/Java-Playground/blob/master/Toy%20Problems/src/DataStructures/DataTypes/StacksDemo.java\">here</a></p>\n</blockquote>\n<h2>Queue</h2>\n<p>Resembles a normal queue on a bus station, the first in line will be the first one to board the bus, the last will obviously be the last. It is based on a first come first serve\nHas 2 major operations:</p>\n<ul>\n<li>Dequeue</li>\n<li>-</li>\n</ul>\n<h2>LinkedLists</h2>\n<p>Data arranged in a list with the first element being the <strong>head</strong> having the node and a pointer. The pointer points to another node and so on and so forth until it reaches the last element with a node that is null. The last element is known as the <strong>tail</strong>\nThe node consists of where the current data is held and where the next data is held.</p>","frontmatter":{"title":"Abstract Data Types","subtitle":"All about abstract data types","date":"July, 12, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"abstract-data-type.jpg","thumbnail":"abstract-data-type.jpg","teaser":"abstract-data-type.jpg","credit":null,"creditlink":null},"path":"/tech/abstract-data-types","tags":["data","arrays","lists","stacks","queues"],"excerpt":"There are several Abstract data types that are used in Java, this is a brief overview of some of them"}}},"pageContext":{"prev":{"html":"<p>The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.\nSo, let's return to some algorithms and see if we learned anything.\nConsider this simple procedure that sums a list (of numbers, we assume):\nprocedure sum(list)\ntotal = 0\nfor i from 0 to length(list)-1\ntotal += list[i]\nreturn total\nFirst: is the algorithm correct? Does it solve the problem specified?\nSecond: is it fast?\nTo evaluate the running time of an algorithm, we will simply ask how many “steps” it takes.\nIn this case, we can count the number of times it runs the += line.\nFor a list with (n) elements, it takes (n) steps.\nOr is counting the += line the right thing to do?\nWhen implementing the for loop, each iteration requires an add (for the loop index) and a comparison (to check the exit condition). We should count those.\nAlso, the variable initialization and return steps.\nSo, (3n+2) steps.\nBut, not all of those steps are the same.\nHow long does an x86 ADD instruction take compared to a CMP or RET instruction?\nWill the compiler keep both i and total in registers, or will one/both be in RAM? (A factor of ~10 difference.)\nHow do those instructions interact in the pipeline? Which can be sent through parallel pipelines in the processor and executed concurrently?\nThe answer to those is simple: I don't know and you don't either.\nThat's part of the reason we're asking about algorithms, not programs.\nBut both (n) and (3n+2) are perfectly reasonable proposals for the answer.\nDeciding between them requires more knowledge about the actual implementation details than we have.\nGood thing we have the function growth notation.\nRemember: this is easy for (n=5) elements. A good or bad algorithm will both be fast then.\nWe want to know how the algorithm behaves for large (n).\nFinally our answer: the sum procedure has running time (\\Theta(n)).\nWe'll say that this algorithm has time complexity (\\Theta(n)), or “runs in linear time”.\nBoth (n) and (3n+2) are (\\Theta(n)), and so is any other “exact” formula we could come up with.\nThe easy answer (count the += line) was just as correct as the very careful one.\nThe big-Θ notation hides all of the details we can't figure out anyway.\nAnother example: print out the sum of each two numbers in a list.\nThat is, given the list [1,2,3,4,5], we want to find 1+2, 1+3, 1+4, 1+5, 2+3, 2+4,….\nPseudocode:\nprocedure sum<em>pairs(list)\nfor i from 0 to length(list)-2\nfor j from i+1 to length(list)-1\nprint list[i] + list[j]\nFor a list with (n) elements, the for j loop iterates (n-1) times when it is called with i==0, then (n-2) times, then (n-3) times,…\nSo, the total number of times the print step runs is [\\begin{align\\</em>} (n-1)+(n-2)+\\cdots+2+1 &#x26;= \\sum<em>{k=1}^{n-1} k\\ &#x26;= \\frac{n(n-1)}{2}\\ &#x26;= \\frac{n^2}{2}-\\frac{n}{2},. \\end{align\\</em>}]\nIf we had counted the initialization of the for loops, counter incrementing, etc, we might have come up with something more like (\\frac{3}{2}n^2 + \\frac{1}{2}n + 1).\nEither way, the answer we give is that it takes (\\Theta(n^2)) steps.\nOr, the algorithm “has time complexity (\\Theta(n^2))” or “has (\\Theta(n^2)) running time” or “has quadratic running time”.\nThe lesson: when counting running time, you can be a bit sloppy.\nWe only need to worry about the inner-most loop(s), not the number of steps in there, or work in the outer levels.\n… because they are going to disappear anyway as constant factors and lower-order terms when they go into a big-O/Ω/Θ anyway.\nAverage and Worst Case</p>\n<p>Consider a linear search: we want to find an element in a list and return its (first) position, or -1 if it's not there.\nprocedure linear_search(list, value)\nfor i from 0 to length(list)-1\nif list[i] == value\nreturn i\nreturn -1\nHow many steps there?\nThe answer is: it depends.\nIf the thing we're looking for is in the first position, it takes (\\Theta(1)) steps.\nIf it's at the end, or not there, it takes (\\Theta(n)) steps.\nThe easiest thing to count is usually the worst case: what is the maximum steps required for any input of size (n)?\nThe worst case is that we go all the way to the end of the list, but don't find it and return -1.\nThe only line that makes sense to count here is the if line. It's in the inner-most loop, and is executed for every iteration.\nWe could also count the number of comparisons made: the == and the implicit comparison in the for loop.\nThat is either (n) or (2n+1) steps, so (\\Theta(n)) complexity.\nThe other useful option is the average case: what is the average steps required over all inputs of size (n)?\nMuch harder to calculate, since you need to consider every possible input to the algorithm.\nEven if we assume the element is found, the possible number of comparisons are:\nFound in position Comparisons\n1 2\n2 4\n⋮ ⋮\n(n) (2n)\nOn average, the number of comparisons is: [\\frac{2+4+\\cdots+2n}{n} = n+1,.]\nAgain, we have (\\Theta(n)) complexity.\n… but it's a good thing we checked. Some algorithms are different.\nGood/bad times</p>\n<p>We have said that these running times are important when it comes to running times of algorithm.\nBut we are throwing away a lot of information when we look only at big-O/Ω/Θ.\nThe lower-order terms must mean something.\nThe leading constants definitely do.\nAssuming one million operations per second, this is the approximate running time of an algorithm given running time, with an input of size (n):\n(n) (\\log<em>2 n) (n) (n\\log</em>2 n) (n^2) (n^{3}) (2^n)\n(10) 3.3 μs 10 μs 33 μs 100 μs 1 ms 1 ms\n(10^2) 6.6 μs 100 μs 664 μs 10 ms 1 s (4\\times 10^{16}) years\n(10^4) 13 μs 10 ms 133 ms 1.7 minutes 11.6 days (10^{2997}) years\n(10^6) 20 μs 1 s 20 s 11.6 days 32000 years (10^{300000}) years\nMaybe that gives a little idea why we'll only worry about complexity\n… at least at first.\nA summary:\nIf you can get (O(\\log n)) life is good: hand it in and go home.\n(O(n\\log n)) is pretty good: hard to complain about it.\n(O(n^k)) could be bad, depending on (k): you won't be solving huge problems. These are polynomial complexity algorithms for (k\\ge 1).\n(\\Omega(k^n)) is a disaster: almost as bad as no algorithm at all if you have double-digit input sizes. These are exponential complexity algorithms for (k\\gt 1).\nSee also: Numbers everyone should know\nA problem that has a polynomial-time algorithm is called tractable.\nNo polynomial time algorithm: intractable.\nThere is a big category of problems that nobody has a polynomial-time algorithm for, but also can't prove that none exists: the NP-complete problems.\nSome examples: Boolean satisfiability, travelling salesman, Hamiltonian path, many scheduling problems, Sudoku (size (n)).\nIf you have an algorithm with a higher complexity than necessary, no amount of clever programming will make up for it.\nNo combination of these will make a (O(n^2)) algorithm faster than an (O(n\\log n)): faster language, better optimizer, hand-optimization of code, faster processor.\nImportant point: the complexity notations only say things about large (n).\nIf you always have small inputs, you might not care.\nAlgorithms with higher complexity class might be faster in practice, if you always have small inputs.\ne.g. Insertion sort has running time (\\Theta(n^2)) but is generally faster than (\\Theta(n\\log n)) sorting algorithms for lists of around 10 or fewer elements.\nThe most important info that the complexity notations throw away is the leading constant.\nThere is a difference between (n^2) instructions and (100n^2) instructions to solve a problem.\nOnce you have the right big-O, then it's time to worry about the constants.\nThat's what clever programming can do.\nWhen we're talking about algorithms (and not programming), the constants don't usually matter much.\nIt's rare to have an algorithm with a big leading constant.\nSo it's not really possible to decide between the algorithms.\nUsually it's a choice between (4n\\log n) or (5n\\log n): you probably have to implement, compile, and profile to decide for sure.\nExample: sorting algorithms. There are several algorithms to sort a list/array.\nInsertion/Selection/Bubble Sorts: (\\Theta(n^2)).\nMerge/Heap Sorts: (\\Theta(n\\log n)).\nQuicksort: (\\Theta(n\\log n)) average case but (very rarely) (\\Theta(n^2)) worst case.\nBut quicksort is usually faster in practice.\n… except when it isn't.\nSeveral recent languages/libraries have implemented a heavily-optimized mergesort (e.g. Python, Perl, Java ≥JDK1.3, Haskell, some STL implementations) instead of Quicksort (C, Ruby, some other STL implementations).\nSpace Complexity</p>\n<p>We have only been talking about running time/speed so far.\nIt also makes good sense to talk about the complexity of other things.\nMost notably, memory use by an algorithm.\nAn algorithm that uses (\\Theta(n^{3})) space is bad. Maybe as bad as (\\Theta(n^{3})) time.\nAn algorithm that uses (O(1)) extra space (in addition to space needed to store the input) is called in-place.\ne.g. selection sort is in-place, but mergesort ((\\Theta(n)) extra space) and Quicksort ((\\Theta(\\log n)) extra space, average case) aren't.</p>","id":"aa82c0c8-f22d-5170-8279-77e177d15931","timeToRead":6,"frontmatter":{"title":"Complexity of Algorithms","subtitle":"Are algorithms that complex?","excerpt":"The whole point of the big-O/Ω/Θ stuff was to be able to say something useful about algorithms.","path":"/tech/algorithm-complexity","category":"tech","date":"July 17, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"algorithm-complexity.jpg","thumbnail":"algorithm-complexity.jpg","teaser":"algorithm-complexity.jpg","credit":null,"creditlink":null},"tags":["complexity of algorithms","algorithms"],"published":true}},"next":{"html":"<p>Algorithms are set to achieve a certain outcome. Sorting algoritms follow numerical order and usually has random access to an array. This means that it can access random indexes of an array to perform a sort.</p>\n<h1>Bubble Sort</h1>\n<p>Convinient for small data sets. Consider an array of elements such that:\nint[] arr = new int[]{14,33,27,10,33,19,42,44};\nThe array is not sorted. To sort this array, we compare 2 values at a time.\nSteps:</p>\n<ul>\n<li>compare 14 and 33, since 14 is less than 33, it remains in place</li>\n<li>next we compare 33 and 27, 27 is less than 33, hence we have to swap postions such that;\n<code>arr[1]</code> = 33 takes the place of <code>arr[2]</code> and vice versa.\nThe array now becomes:\n{14,27,33,10,33,19,48,44}</li>\n<li>The next comparison is now between 33 and 10. 10 is less than 33, so we swap positions as before.</li>\n<li>The process continues until all the elements in the array follow natural ordering.</li>\n</ul>\n<p>This is obviously convenient for small data sets as we are comparing 2 elements at a time. If the array was of length 50, it would take much longer to complete and would be inefficient. This is not what an ideal algorithm should achieve.</p>\n<pre><code class=\"language-java\">    /**\n     * @implNote\n     * set flag to true to begin first pass, initialize the step variable, create the temp variable\n     * within while loop, set the flag to false awaiting first pass\n     * record the steps taken in a variable called steps\n     * holds the temp value for the current element in array\n     * assign the current position of the array to the next element\n     * assign the next position of the array to the temp value\n     * set the flag to true to allow continuing of loop, record the steps taken\n     * @param toSort array to perform the bubble sort\n     * @return toSort the sorted array\n     * */\n    public static int[] bubbly(int[] toSort){\n        boolean flag = true;\n        int steps = 0;\n        int temp;\n        while(flag){\n            flag = false;\n            for(int x = 0; x &#x3C; toSort.length-1;x++){\n                if(toSort[x] > toSort[x+1]){\n                    steps++;\n                    temp = toSort[x];\n                    toSort[x] = toSort[x+1];\n                    toSort[x+1] = temp;\n                    flag = true;\n                }\n\n            }\n        }\n        System.out.println(String.valueOf(steps)+ \" steps taken\");\n        return toSort;\n    }\n\n    public static void main(String[] args){\n        int[] arr = new int[]{18,29,1,100,17};\n        int[] arrTwo = new int[]{100,16,33,48,60,21,80};\n        /*Bubble sort Ascending*/\n        System.out.println(Arrays.toString(bubbly(arr)));\n        System.out.println(Arrays.toString(bubbly(arrTwo)));\n        /*5 steps taken\n        [1, 17, 18, 29, 100]\n        9 steps taken\n        [16, 21, 33, 48, 60, 80, 100]*/\n    }\n</code></pre>\n<blockquote>\n<p>Demonstration of a bubble sort algorithm, sorting elements in ascending order</p>\n</blockquote>\n<p>Code snippet right <a href=\"https://github.com/BrianLusina/Java-Playground/blob/master/Toy%20Problems/src/SortingAlgorithms/BubbleSortDemo.java\">here</a>.</p>\n<h1>Merge Sort</h1>\n<p>Uses the <em>divide and conquer rule</em> it divides a problem into smaller parts until it reaches the simplest form possible. Then it rejoins the divided elements in a sorted format until it is a full list again. This sorting algorithm is mostly used in arrays.</p>\n<p>Consider this example\n27 10 12 25 34 16 15 31\ndivide it into two parts\n27 10 12 25 34 16 15 31\ndivide each part into two parts\n27 10 12 25 34 16 15 31\ndivide each part into two parts\n27 10 12 25 34 16 15 31</p>\n<p>merge (cleverly-!) parts</p>\n<p>10 27 12 25 16 34 15 31\nmerge parts\n10 12 25 27 15 16 31 34\nmerge parts into one\n10 12 15 16 25 27 31 34</p>\n<h1>Insertion Sort</h1>","id":"48df5b4a-6bf5-576e-a6a1-8408714e9881","timeToRead":2,"frontmatter":{"title":"Sorting Algorithms","subtitle":"Brief overview of some sorting algorithms","excerpt":"Algorithms are set to achieve a certain outcome. Sorting algoritms follow numerical order and usually has random access to an array. This means that it can access random indexes of an array to perform a sort.","path":"/tech/sorting-algorithms","category":"tech","date":"July 12, 2016","author":{"name":"Brian Lusina","link":"/brian_lusina","avatar":"brian_lusina.jpg"},"image":{"feature":"sorting-algorithms.png","thumbnail":"sorting-algorithms.png","teaser":"sorting-algorithms.png","credit":null,"creditlink":null},"tags":["algorithms","sorting algorithms","array"],"published":true}}}}}